
@article{shortgpt_Men_Xu_Zhang_Wang_Lin_Lu_Han_Chen,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{rm_Samragh_Farajtabar_Mehta_Vemulapalli_Faghri_Naik_Tuzel_Apple,
  title={Weight subcloning: direct initialization of transformers using larger pretrained ones},
  author={Samragh, Mohammad and Farajtabar, Mehrdad and Mehta, Sachin and Vemulapalli, Raviteja and Faghri, Fartash and Naik, Devang and Tuzel, Oncel and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2312.09299},
  year={2023}
}

@article{slicegpt_ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}

@article{Stephens_arm_vector_extension,
  title={The ARM scalable vector extension},
  author={Stephens, Nigel and Biles, Stuart and Boettcher, Matthias and Eapen, Jacob and Eyole, Mbou and Gabrielli, Giacomo and Horsnell, Matt and Magklis, Grigorios and Martinez, Alejandro and Premillieu, Nathanael and others},
  journal={IEEE micro},
  volume={37},
  number={2},
  pages={26--39},
  year={2017},
  publisher={IEEE}
}

@article{intel_cpu_Shen_Chang_Dong_Luo_Meng_2023,   title={Efficient LLM Inference on CPUs},  author={Shen, Haihao and Chang, Hanwen and Dong, Bo and Luo, Yu and Meng, Hengyu}, 
year={2023}, 
month={Nov}, 
journal={arXiv preprint arXiv:2311.00502},
language={en-US}  }

@misc{k_quants_github_repository,
  author = {ggerganov},
  title = {llama.cpp k-quants},
  howpublished = {\url{https://github.com/ggerganov/llama.cpp/pull/1684}},
  year = {2024},
  note = {Accessed: Date}
}

@misc{arm_cpu_llms,
  author = {Dibakar Gope, David Mansell, Ian Bratt},
  title = {Large Language Models on CPUs},
  howpublished = {\url{https://www.pccluster.org/ja/event/pccc23/data/PCCC23_1207_01_arm.pdf}},
  year = {2023},
  note = {Accessed: Date}
}

@article{qwen_1_8_bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@misc{saxena2023prompt_lookup,
    title = {Prompt Lookup Decoding},
    author = {Apoorv Saxena},
    year = {2023},
    month = {November},
    url = {https://github.com/apoorvumang/prompt-lookup-decoding/}
}

@misc{lm_eval_harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

 @article{code_llama_Rozière_Gehring_Gloeckle_Sootla_Gat_Tan_Adi_Liu_Remez_Rapin_et,  
 title={CodeLlama: Open Foundation Models for Code}, 
 author={Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Ferrer, CristianCanton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel}, 
 language={en-US},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
 }

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{li2024transformer_lite,
  title={Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs},
  author={Li, Luchang and Qian, Sheng and Lu, Jie and Yuan, Lunxi and Wang, Rui and Xie, Qin},
  journal={arXiv preprint arXiv:2403.20041},
  year={2024}
}

@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={arXiv preprint arXiv:2312.15234},
  year={2023}
}

@article{self_attention_Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017,   title={Attention is All you Need},  journal={Neural Information Processing Systems,Neural Information Processing Systems},  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, AidanN. and Kaiser, Lukasz and Polosukhin, Illia},  year={2017},  month={Jun},  language={en-US}  }

@inproceedings{liu2023dejavu,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR}
}

@article{alizadeh2023llm_flash,
  title={Llm in a flash: Efficient large language model inference with limited memory},
  author={Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C and Rastegari, Mohammad and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2312.11514},
  year={2023}
}